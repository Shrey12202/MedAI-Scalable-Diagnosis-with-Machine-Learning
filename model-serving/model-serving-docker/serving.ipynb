{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "156329d6",
   "metadata": {},
   "source": [
    "Run this to install required services:\n",
    "\n",
    "pip install mlflow boto3 torch fastapi uvicorn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b61b37ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import torch\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from fastapi.responses import JSONResponse\n",
    "import nest_asyncio\n",
    "import uvicorn\n",
    "from fastapi import UploadFile, File\n",
    "from fastapi.responses import JSONResponse\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3dcb7a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()  # Fixes event loop conflicts in Jupyter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12b52075",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shaba\\AppData\\Local\\Temp\\ipykernel_8552\\698748256.py:8: DeprecationWarning: \n",
      "        on_event is deprecated, use lifespan event handlers instead.\n",
      "\n",
      "        Read more about it in the\n",
      "        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).\n",
      "        \n",
      "  @app.on_event(\"startup\")\n"
     ]
    }
   ],
   "source": [
    "app = FastAPI()\n",
    "\n",
    "class InputData(BaseModel):\n",
    "    inputs: list\n",
    "\n",
    "model = None  # global to be loaded in startup\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "def load_latest_model():\n",
    "    global model\n",
    "    os.environ[\"MLFLOW_TRACKING_URI\"] = \"http://localhost:5000\"\n",
    "    os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"http://localhost:9000\"\n",
    "    os.environ[\"AWS_ACCESS_KEY_ID\"] = \"minioadmin\"\n",
    "    os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"minioadmin\"\n",
    "\n",
    "    from mlflow.tracking import MlflowClient\n",
    "    client = MlflowClient()\n",
    "    exp = client.get_experiment_by_name(\"chexpert-jupyter\")\n",
    "    if exp is None:\n",
    "        raise ValueError(\"Experiment 'chexpert' not found.\")\n",
    "\n",
    "    latest_run = sorted(\n",
    "        client.search_runs(exp.experiment_id),\n",
    "        key=lambda r: r.info.start_time,\n",
    "        reverse=True\n",
    "    )[0]\n",
    "\n",
    "    print(f\"ðŸ”„ Loading model from run {latest_run.info.run_id}\")\n",
    "    model_uri = f\"runs:/{latest_run.info.run_id}/final_model\"\n",
    "    model = mlflow.pytorch.load_model(model_uri)\n",
    "    model.eval()\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "async def predict_image(file: UploadFile = File(...)):\n",
    "    try:\n",
    "        image_bytes = await file.read()\n",
    "        image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
    "\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        input_tensor = transform(image).unsqueeze(0)\n",
    "\n",
    "        # ðŸ”¹ Fix: Move to model's device\n",
    "        device = next(model.parameters()).device\n",
    "        input_tensor = input_tensor.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_tensor)\n",
    "            probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            return {\"probabilities\": probs.tolist()}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"ðŸ”¥ Prediction error:\", e)\n",
    "        return JSONResponse(status_code=500, content={\"error\": str(e)})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbe54b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [8552]\n",
      "INFO:     Waiting for application startup.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Loading model from run 26c8da488ee5473ebd787fce6eda55c2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 86.49it/s]  \n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:62466 - \"POST /predict HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:62470 - \"POST /predict HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
