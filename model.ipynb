{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b563937",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torchvision import models\n",
    "from torch.utils.data import Dataset\n",
    "import mlflow\n",
    "from prometheus_client import start_http_server, Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "004c71e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Config\n",
    "# -----------------------\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_CLASSES = 14\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "TRAIN_CSV = \"sample_train.csv\"\n",
    "TEST_CSV = \"sample_test.csv\"\n",
    "IMG_ROOT = \".\"  # Current dir includes CheXpert-v1.0/\n",
    "\n",
    "LABELS = [\n",
    "    \"Enlarged Cardiomediastinum\", \"Cardiomegaly\", \"Lung Opacity\", \"Lung Lesion\",\n",
    "    \"Edema\", \"Consolidation\", \"Pneumonia\", \"Atelectasis\", \"Pneumothorax\",\n",
    "    \"Pleural Effusion\", \"Pleural Other\", \"Fracture\", \"Support Devices\", \"No Finding\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df2ff4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Dataset\n",
    "# -----------------------\n",
    "class CheXpertDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.df[LABELS] = self.df[LABELS].fillna(0).replace(-1, 0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.root_dir, row[\"Path\"])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        labels = torch.tensor(row[LABELS].values.astype(\"float32\"))\n",
    "        return image, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e4eb650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Model\n",
    "# -----------------------\n",
    "class CheXpertModel(nn.Module):\n",
    "    def __init__(self, num_classes=14):\n",
    "        super().__init__()\n",
    "        base = models.densenet121(pretrained=True)\n",
    "        in_features = base.classifier.in_features\n",
    "        base.classifier = nn.Linear(in_features, num_classes)\n",
    "        self.model = base\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e108234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# AUC Evaluation\n",
    "# -----------------------\n",
    "# def compute_auc(y_true, y_pred):\n",
    "#     try:\n",
    "#         aucs = [roc_auc_score(y_true[:, i], y_pred[:, i]) for i in range(y_true.shape[1])]\n",
    "#         return sum(aucs) / len(aucs)\n",
    "#     except Exception:\n",
    "#         return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5d0f37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Transforms\n",
    "# -----------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b0e7989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Load Data\n",
    "# -----------------------\n",
    "train_ds = CheXpertDataset(TRAIN_CSV, IMG_ROOT, transform)\n",
    "test_ds = CheXpertDataset(TEST_CSV, IMG_ROOT, transform)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7694693f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/30 15:35:23 INFO mlflow.tracking.fluent: Experiment with name 'model_training_experiment' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<prometheus_client.exposition.start_wsgi_server.<locals>.TmpServer at 0x19d18141650>,\n",
       " <Thread(Thread-3 (serve_forever), started daemon 25964)>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ðŸ”¹ MLflow Experiment Setup\n",
    "mlflow.set_experiment(\"model_training_experiment\")\n",
    "\n",
    "# ðŸ”¹ Prometheus Metric Setup\n",
    "training_time = Summary('training_duration_seconds', 'Time spent training model')\n",
    "start_http_server(8001)  # Prometheus will scrape metrics from http://localhost:8001/metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee5e48bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train Loss: 0.6641\n",
      "âœ… Saved new best model at epoch 1 (Loss: 0.6641)\n",
      "[Epoch 2] Train Loss: 0.5336\n",
      "âœ… Saved new best model at epoch 2 (Loss: 0.5336)\n",
      "[Epoch 3] Train Loss: 0.4333\n",
      "âœ… Saved new best model at epoch 3 (Loss: 0.4333)\n",
      "[Epoch 4] Train Loss: 0.3676\n",
      "âœ… Saved new best model at epoch 4 (Loss: 0.3676)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/30 15:51:52 WARNING mlflow.utils.requirements_utils: Found torch version (2.5.1+cu121) contains a local version label (+cu121). MLflow logged a pip requirement for this package as 'torch==2.5.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Train Loss: 0.3175\n",
      "âœ… Saved new best model at epoch 5 (Loss: 0.3175)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/30 15:51:58 WARNING mlflow.utils.requirements_utils: Found torchvision version (0.20.1+cu121) contains a local version label (+cu121). MLflow logged a pip requirement for this package as 'torchvision==0.20.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2025/04/30 15:51:58 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# Train\n",
    "# -----------------------\n",
    "model = CheXpertModel(NUM_CLASSES).to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2)\n",
    "\n",
    "best_model_path = \"best_model.pth\"\n",
    "\n",
    "\n",
    "@training_time.time()\n",
    "def train_model():\n",
    "    best_loss = float('inf')  # now inside the function\n",
    "    best_model_path = \"best_model.pth\"\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_param(\"epochs\", EPOCHS)\n",
    "        mlflow.log_param(\"batch_size\", BATCH_SIZE)\n",
    "\n",
    "        for epoch in range(EPOCHS):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "\n",
    "            for imgs, labels in train_loader:\n",
    "                imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(imgs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            print(f\"[Epoch {epoch+1}] Train Loss: {avg_loss:.4f}\")\n",
    "            scheduler.step(avg_loss)\n",
    "\n",
    "            if avg_loss < best_loss:\n",
    "                best_loss = avg_loss\n",
    "                torch.save(model.state_dict(), best_model_path)\n",
    "                print(f\"âœ… Saved new best model at epoch {epoch+1} (Loss: {avg_loss:.4f})\")\n",
    "\n",
    "        # Make sure final_accuracy is defined or remove this if you don't calculate it\n",
    "        mlflow.pytorch.log_model(model, \"model\")\n",
    "\n",
    "train_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "112d6e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shaba\\AppData\\Local\\Temp\\ipykernel_36960\\2245467968.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Test Accuracy (avg over all labels): 0.8143\n",
      "\n",
      "ðŸ“Š Ground Truth vs Predictions (first 3 images):\n",
      "\n",
      "Image 1:\n",
      "Labels (GT):      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
      "Predictions (bin): [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Image 2:\n",
      "Labels (GT):      [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
      "Predictions (bin): [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Image 3:\n",
      "Labels (GT):      [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
      "Predictions (bin): [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Image 4:\n",
      "Labels (GT):      [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "Predictions (bin): [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "\n",
      "Image 5:\n",
      "Labels (GT):      [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "Predictions (bin): [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# Evaluation\n",
    "# -----------------------\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in test_loader:\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        outputs = torch.sigmoid(model(imgs)).cpu()\n",
    "        all_preds.append(outputs)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "all_preds = torch.cat(all_preds).numpy()\n",
    "all_labels = torch.cat(all_labels).numpy()\n",
    "\n",
    "# Convert probabilities to binary predictions (threshold = 0.5)\n",
    "preds_binary = (all_preds >= 0.5).astype(int)\n",
    "\n",
    "# Compute accuracy (macro across all labels)\n",
    "correct = (preds_binary == all_labels).sum()\n",
    "total = preds_binary.size\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f\"\\nâœ… Test Accuracy (avg over all labels): {accuracy:.4f}\")\n",
    "mlflow.log_metric(\"test_accuracy\", accuracy)\n",
    "\n",
    "\n",
    "# Show ground truth vs prediction for a few samples\n",
    "print(\"\\nðŸ“Š Ground Truth vs Predictions (first 3 images):\")\n",
    "for i in range(min(6, len(all_preds))):\n",
    "    print(f\"\\nImage {i+1}:\")\n",
    "    print(\"Labels (GT):     \", all_labels[i].astype(int).tolist())\n",
    "    print(\"Predictions (bin):\", (all_preds[i] >= 0.5).astype(int).tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2907118",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
